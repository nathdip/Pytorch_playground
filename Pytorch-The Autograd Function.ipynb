{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is autograd?\n",
    "\n",
    "Autograd is one of the most important packages in Pytorch. The autograd package allows us to differentiate all tensors given an operation. Differentiation is a constituent of  backpropagation, which allows us to update the weights in a linear regression or in a Deep Neural Network. Thus autograd allows to smoothly perform backpropagation independent of the operations involved in-between. Let us delve into further details. \n",
    "Key features :\n",
    "1. It is a define by the run framework, which implies that the backpropagation is defined by the code, and every single run can be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The autograd variable\n",
    "\n",
    "The autograd.Variable is the central class of the package. The Variable wraps around tensors and supports nearly all of pytorch tensor operators. Like in TensorFlow one also needs to initialize the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a variable\n",
    "x = Variable(torch.ones(2, 2), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform various operation on the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the y was created as a result of an operation, leads to y acquiring an additional attribute knows as .grad_fn. 'y' has been created due to a particular function and thus it has the attribute. In other words, Variable and Function are actually interconnected. You will use functions to create variables and the created a variable will have attribute of the functions. Howver the variables directly created by the user do not have the function attribute as they have been the result of any pytorch operation. For the example in the above example the variable y has been created as a result of and addition operation and will thus have a grad_fn attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x0000021C9C027FD0>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 39  39\n",
      " 39  39\n",
      "[torch.FloatTensor of size 2x2]\n",
      " Variable containing:\n",
      " 39\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's make more functions\n",
    "z = 3*y*y + 4*y\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform backpropagation\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 5.5000  5.5000\n",
      " 5.5000  5.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print t\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check whether the output is correct or not. We are calculating the gradient $\\frac{d out}{dx}$ at $x = 1$. Now $$out =\\frac{1}{4}\\sum z_{i}$$ where $ z_{i} = 3\\left(x + 2\\right)^2 + 4\\left(x + 2\\right) $. Now $$\\frac{d\\text{ }out}{dx} = \\frac{3}{2}\\left(x + 2\\right) + 1 $$ We should get a $2\\times 2$ tensor with each element having a value 5.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next tutorial I will discuss how to proceed to make a simple neural network with the help of all the tools at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
